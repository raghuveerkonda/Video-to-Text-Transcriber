# Videoâ€‘toâ€‘Text Transcriber

A **Python + Whisper** desktop utility that takes any video (local file *or* URL), extracts its audio with FFmpeg, transcribes it with OpenAIÂ Whisper, and saves a cleanâ€¯`.txt` transcript.

The project is split into two independent scripts so you can keep your existing logic intact:

| Script                     | Role                                                                                                                                                                |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Convertâ€‘videototext.py** | 1â€‘click GUI that does *one* transcription job: download â–¶ extract â–¶ transcribe  (kept exactly as you wrote it)                                                      |
| **manager.py**             | A wrapper that adds powerâ€‘user features (batch mode, duplicate detection, model selection, tidyâ€‘up prompt, etc.) and then calls `Convertâ€‘videototext.py` underneath |

> **ğŸ‘€ Why two files?**â€ƒSo you donâ€™t have to rewrite working code.  `manager.py` simply orchestrates many singleâ€‘file runs of your original script.

---

## âœ¨ Features

### 1. Correct filename handâ€‘off

* After Whisper finishes, `manager.py` renames `temp_audio.txt` to

  ```
  <videoâ€‘name>_transcribed_<YYYYâ€‘MMâ€‘DD_HHâ€‘MMâ€‘SS>.txt
  ```

  so every transcript is traceable and clashâ€‘free.

### 2. Duplicate detection (local **and** URL)

* A tiny JSON db (`conversion_db.json`) stores

  * the **absolute file path** (for local files) **or** canonical URL
  * SHAâ€‘256 hash (local) Â or Â ytâ€‘dlp videoâ€‘ID (URL)
  * the transcript path + timestamp
* When you try the same input again, youâ€™re politely asked whether to open the previous transcript instead of wasting timeÂ and GPU.

### 3. Model / language options

* Pick any official Whisper checkpoints (`tiny`, `base`, `small`, `medium`, `large`) from a dropâ€‘down.
* Choose transcription language (leave blank for autoâ€‘detect).
* **Headsâ€‘up:** larger models download at first use (â‰ˆÂ 1.5â€¯GBÂ â†’Â 6â€¯GB) and need more VRAM/CPU â€‘ select wisely!

### 4. Batch / folder mode

* Point `manager.py` at a folder and it queues **all** video files inside (optionally recursive).  Perfect for lecture playlists or surveillance dumps.

### 5. Rich exception surfacing

* All subprocess and network errors bubble up into Tkinter messageâ€‘boxes with a *copyâ€‘toâ€‘clipboard* details button so users can report issues.

### 6. Postâ€‘run cleanâ€‘up dialog

* After the last file finishes you get a checklist of everything that was downloaded or generated (video, audio, transcript).
* Green âœ”ï¸ = keep,   Red âœ˜ = delete  â†’ one click to tidyâ€‘up.

---

## ğŸ–¥  Installation

```bash
# 1. Clone your repo
$ git clone https://github.com/yourâ€‘handle/videoâ€‘toâ€‘text.git
$ cd videoâ€‘toâ€‘text

# 2. Create env (recommended)
$ python -m venv venv
$ ./venv/Scripts/activate       # Windows

# 3. Install requirements
$ pip install -r requirements.txt
```

### External tools

* **FFmpeg** â€“ add `ffmpeg.exe` to your PATH Â ( `winget install ffmpeg` on Windows)
* **ytâ€‘dlp** â€“ autoâ€‘installed via `requirements.txt`

---

## ğŸš€ Usage

### GUI (normal users)

```bash
python manager.py        # opens GUI
```

### CLI (power users / automation)

```bash
python manager.py  --input D:/Videos  --model small  --lang en  --recursive
```

* `--input`Â Â Â file **or** folder **or** url
* `--model`Â Â `tinyâ”‚baseâ”‚smallâ”‚mediumâ”‚large`
* `--lang`Â Â Â ISOâ€‘639â€‘1 code, blankÂ =Â auto
* `--recursive`Â Â include subâ€‘folders

---

## ğŸ—‚  Project structure

```
videoâ€‘toâ€‘text/
â”œâ”€ Convertâ€‘videototext.py   # your original GUI (unchanged)
â”œâ”€ manager.py               # new wrapper with all bells & whistles
â”œâ”€ requirements.txt         # pip deps (torch, whisper, ytâ€‘dlp, requests, tk, etc.)
â”œâ”€ conversion_db.json       # created on first run
â””â”€ README.md                # youâ€™re reading it âœ…
```

---

## ğŸ¤  Contributing

1. **Fork** the repo & create your feature branch
   `git checkout -b feature/awesome`
2. **Commit** your changes
   `git commit -m "Add awesome feature"`
3. **Push** to the branch
   `git push origin feature/awesome`
4. **Open Pull Request**

Please follow conventionalâ€‘commit style and run `preâ€‘commit` before PRs.

---

## ğŸ“„  License

MIT â€“ do whatever you want, just donâ€™t sue us.
